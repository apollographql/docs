---
title: Response Cache Eviction
id: TN0010
tags: [server, caching, performance]
---

_For a runnable example of this cache eviction solution, see the [Response Cache Eviction](https://github.com/apollosolutions/response-cache-eviction) repo._

Apollo Server’s Full Response Cache plugin (`apollo-server-plugin-response-cache`) caches the results of operations for a period of time (time-to-live or TTL). After that time expires, the results are evicted from the cache and the server will execute the operation again.

In practice, this usually means setting a short TTL so that results don’t become stale. 

As of version `3.7.0`, the Full Response Cache plugin (`apollo-server-plugin-response-cache`) allows customizing cache keys to support new cache eviction patterns. This means it’s easier to use longer cache TTLs and increase the hit rate on your cache by evicting the cached responses when relevant events occur.

## Customizing the Cache Key

This works by defining a custom response cache key by a pattern that can later be searched on in the cache. What this key should be comprised of and how it should be structured depends on the use-case and what search patterns our cache implementation supports. 

### A Note on Cache Key Uniqueness

Keep in mind that each key links to a full response object, so if your key is too generic, you risk potentially returning the wrong data for queries. For example, generating a cache key based solely on the operation name would yield the same responses for all operations with the same name, even if the entire query is different. Make sure your keys are unique for each execution of the incoming operations that returns different data. 

### Defining a Custom Cache Key

As noted above, the `3.7.0` of the Full Response Cache plugin introduced the `generateCacheKey` configuration method. The response from this function will be used as the cache key to store the current query response.

Here’s the method signature:

```typescript
generateCacheKey(
  requestContext: GraphQLRequestContext<Record<string, any>>,
  keyData: unknown,
): string;

```

The [requestContext](https://github.com/apollographql/apollo-server/blob/main/packages/apollo-server-types/src/index.ts#L135-L175) parameter holds data about the running GraphQL request, such as the request / response objects as well as the [context object](https://www.apollographql.com/docs/apollo-server/data/resolvers/#the-context-argument) that is passed to your resolver functions. Any portion of these data objects can be used as part of your cache key. 

The `keyData` parameter can be used to ensure the uniqueness of your key, simply hashing this variable should be enough to generate a unique key per operation. In fact, the [default implementation](https://github.com/apollographql/apollo-server/blob/76caff6ce797e00907d6ed8e2dab870e255340fe/packages/apollo-server-plugin-response-cache/src/ApolloServerPluginResponseCache.ts#L168-L169) just hashes a `JSON.strigify` version of this parameter as the cache key. 

In this example, we prefix the default key with the name of the incoming operation:

```typescript
import { createHash } from 'crypto';

function sha(s: string) {
  return createHash('sha256').update(s).digest('hex');
}

generateCacheKey(requestContext, keyData) {
  const operationName = requestContext.request.operationName ?? 'unnamed';
  const key = operationName + ':' + sha(JSON.stringify(keyData));
  return key;
}
```

An example key for the named operation “MyOpName”: 

```bash
keyv:fqc:MyOpName:e7eed80930547ed4ab4ece81a18955967831ff4c40757eda9bf1f0de84e042f8
```

This approach ensures that all cache keys are unique enough to store unique responses, but gives us a pattern we can leverage to selectively remove cache entries based on our operation names. 

## Evicting Cache Entries

There are two main strategies for evicting response cache entries: manually evicting from a shell prompt, or in response to some event, like a mutation. 

Actually removing entries from the cache once a custom cache key is being used will depend on your caching backend, as each offer different ways to list and remove keys. We’ll explore both options using Redis. 

### A Manual Solution

It may be sufficient for your use-case to only need to evict portions of the cache in one-off scenarios, ie, debugging or testing. In such a scenario, simply having a custom cache key pattern in place and shell access to your Redis instance could be enough. 

Here’s an example of removing all keys in a Redis instance that match a given pattern:

```bash
redis-cli --raw KEYS "$PATTERN" | xargs redis-cli del
```

This command will list out every key matching any glob-style "$PATTERN" and remove them one by one. 

Here’s an example using the operation name prefix pattern described above to remove all entries with unnamed operations:

```bash
redis-cli --raw KEYS "keyv:fqc:unnamed*" | xargs redis-cli del
```

[The Redis docs for the KEYS command](https://redis.io/commands/keys/) recommend NOT using the `KEYS` function in production application code and only execute against production with “extreme care”. Redis specifically recommends using `SCAN` instead for this type of thing, which is described in the Event Based Solution section.

The utility of this approach will depend on the number of records stored in your cache and how performant the pattern search is. It will also depend on the number of records that need to be removed. If your searches are scanning and/or returning millions of records, this approach probably should be avoided in a production environment. 

### An Event Based Solution

Most other use-cases need to evict cache entries in response to certain events. The [Response Cache Eviction](https://github.com/apollosolutions/response-cache-eviction) repo, mentioned above, has a full walk-through of evicting certain operation responses from the cache when a specific mutation is executed.

Redis clients offers no way to batch delete entries based on a pattern (that we’ve found at least). Our event based solution needs to do a similar algorithm: Look up keys by a pattern, then remove those keys. 

The following snippet is from [the repo mentioned above](https://github.com/apollosolutions/response-cache-eviction): 

```tsx
import { createClient, RedisClientType } from 'redis';

const deleteByPrefix = async (prefix: string) => {
  const client = createClient({ url: 'redis://localhost:6379' })
  await client.connect()

  const scanIterator = client.scanIterator({
    MATCH: `keyv:fqc:${prefix}*`,
    COUNT: 2000, 
  })

  let keys = []

  for await (const key of scanIterator) {
    keys.push(key)
  }

  if (keys.length > 0) {
    client.del(keys)
  }

  return keys
}
```

This solution uses the `scanIterator` function (which uses the `SCAN` Redis function) to scan through cache entries in a memory efficient way, as opposed to the `KEYS` method mentioned above. The `SCAN` method is more appropriate to use in a production environment. 

The `deleteByPrefix` method can be added to your context object and then executed in your mutation resolvers to remove certain operations from the cache. 

Again a full working example of this is available in the [Response Cache Eviction](https://github.com/apollosolutions/response-cache-eviction) repo.

## Final Thoughts

Either of the eviction solutions mentioned above should be used with caution. Make sure you have an understanding of the sorts of effects that your set up will have on your cache. Its a good idea to monitor your caching server when testing our your different use-cases to ensure that you aren’t overloading your cache.
